# -*- coding: utf-8 -*-
"""MovieRecommendationSystem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ze6DYmVCxUkscwsdDjpnTzvLRf6QJP-4

## Import Semua Packages/Library yang Digunakan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import gdown
import zipfile
import os
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import mean_absolute_error, mean_squared_error

"""## Data Understanding

Mengambil Data dari Google drive: Menggunakan library gdown untuk mendownload dataset movie yang berisi list film dan rating pengguna.
"""

# Download file ZIP dari Google Drive
file_id = "1LGWOVE41B7P_tnyUXzRaHikiZbJmJj-a"  # Ganti dengan ID file ZIP Anda
url = f"https://drive.google.com/uc?id={file_id}"
output_zip = "dataset.zip"

gdown.download(url, output_zip, quiet=False)

# Ekstrak file ZIP
output_folder = "datasets"
with zipfile.ZipFile(output_zip, 'r') as zip_ref:
    zip_ref.extractall(output_folder)

# Cek file yang diekstrak
print("Files extracted:")
for file_name in os.listdir(output_folder):
    print(file_name)

"""Melihat isi dari movies dan ratings"""

# Tentukan path untuk masing-masing file CSV
movies = os.path.join(output_folder, "movies.csv")
ratings = os.path.join(output_folder, "ratings.csv")

# Baca dataset dari file CSV
movies_data = pd.read_csv(movies)
ratings_data = pd.read_csv(ratings)

# Tampilkan beberapa baris pertama dari salah satu dataset
print("Movies Dataset:")
display(movies_data.head())

print("\nRatings Dataset:")
display(ratings_data.head())

"""Mengecek mising value pada data dengan fungsi isnull() dan mengecek jumlahnya dengan fungsi sum()"""

print("Movies Dataset:")
display(movies_data.isnull().sum())

print("\nRatings Dataset:")
display(ratings_data.isnull().sum())

"""Mengecek duplikat data menggunakan fungsi duplicated() dan menghitung jumlahnya dengan fungsi sum()"""

print("Movies Dataset:")
display(movies_data.duplicated().sum())

print("\nRatings Dataset:")
display(ratings_data.duplicated().sum())

"""Mengecek detail data movies dan ratings menggunakan fungsi info()"""

print("Movies Dataset:")
display(movies_data.info())

print("\nRatings Dataset:")
display(ratings_data.info())

"""Mengecek dan menampilkan daftar film dan genre"""

print('Jumlah film: ', len(movies_data.movieId.unique()))
print('Judul Film: ', movies_data.title.unique())

# Pecah genres berdasarkan '|'
all_genres = movies_data['genres'].str.split('|').explode()

# Hitung jumlah genres unik
unique_genres = all_genres.unique()
print('\nJumlah genres: ', len(unique_genres))

# Tampilkan genres unik
print('Daftar genres: ', unique_genres)

"""Mengecek jumlah user, rating, dan jumlah film yang diberikan rating"""

print('Jumlah user: ', len(ratings_data.userId.unique()))
print('Jumlah data rating: ', len(ratings_data.rating))
print('Jumlah film: ', len(ratings_data.movieId.unique()))

"""Mengecek distribusi rating pada data, dapat dilihat bahwa rating tertinggi yaitu 3 dan 4"""

# Distribusi rating
plt.figure(figsize=(8, 6))
ratings_data['rating'].value_counts().sort_index().plot(kind='bar', color='skyblue')
plt.title('Distribusi Rating Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah Pengguna')
plt.xticks(rotation=0)
plt.show()

"""Selanjutnya, kita akan mengecek genre yang paling banyak muncul. disini Drama dan Comedy menjadi genre yang paling banyak muncul di banding yang lain"""

# Membagi genre yang dipisahkan oleh '|'
all_genres = movies_data['genres'].str.split('|', expand=True).stack().value_counts()

# Tampilkan 10 genre terbanyak
print(all_genres.head(10))

# Visualisasi 10 genre terbanyak
plt.figure(figsize=(10, 6))
all_genres.head(10).plot(kind='bar', color='lightcoral')
plt.title('10 Genre Paling Banyak Muncul')
plt.xlabel('Genre')
plt.ylabel('Jumlah Film')
plt.xticks(rotation=45)
plt.show()

"""## Data Preparation

Menghapus film yang tidak mempunyai genre atau 'no genres listed', karena tidak berpengaruh signifikan pada model
"""

print("Jumlah data sebelum:", len(movies_data))
movies_data = movies_data[movies_data['genres'] != '(no genres listed)']
print("Jumlah data sesudah:", len(movies_data))

"""Dikarenakan ukuran dataset yang besar dan memory yang terbatas maka kita akan melakukan undersampling yaitu hanya mengambil beberapa sampel data sebanyak 10000 data secara acak"""

# Ambil 10.000 data dari Ratings Dataset
ratings_sampled = ratings_data.sample(n=10000, random_state=42)

print(f"Jumlah data rating setelah sampling: {len(ratings_sampled)}")
print(f"Jumlah user: {ratings_sampled['userId'].nunique()}")
print(f"Jumlah film: {ratings_sampled['movieId'].nunique()}")

"""Sesuaikan data movie berdasarkan hasil undersampling dari rating_sampled"""

# Filter Movies Dataset berdasarkan movieId yang ada di ratings_sampled
sampled_movie_ids = ratings_sampled['movieId'].unique()
movies_sampled = movies_data[movies_data['movieId'].isin(sampled_movie_ids)]

print(f"Jumlah film setelah sampling: {len(movies_sampled)}")

"""## Modeling

Selanjutnya adalah tahap modeling dengan teknik Content-based Filtering dan Collaborative Filtering.

- Content-based Filtering menggunakan representasi fitur item dengan metode seperti TF-IDF untuk mengukur kesamaan antara item dan preferensi pengguna, sehingga dapat memberikan rekomendasi item yang relevan berdasarkan konten.
- Collaborative Filtering menggunakan Matrix Factorization atau model berbasis Neural Network, seperti Multilayer Recommender Networks (Mural), untuk menganalisis pola interaksi antar pengguna dan item, serta memberikan rekomendasi berdasarkan preferensi pengguna yang serupa.

### Content-Based Filtering

Memisahkan genre menjadi format yang bisa diolah
"""

movies_sampled['genres'] = movies_sampled['genres'].str.split('|')

"""Gabungkan genres menjadi string tunggal untuk setiap film, kemudian Konversi genres ke dalam bentuk vektor TF-IDF"""

movies_sampled['genres_str'] = movies_sampled['genres'].apply(lambda x: ' '.join(x))

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(movies_sampled['genres_str'])

"""Menghitung similarity antara film berdasarkan genre"""

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

"""Membuat fungsi recommend_movies untuk mencari film-film yang memiliki genre paling mirip dengan film yang diberikan sebagai input (dalam contoh ini, "Toy Story (1995)")"""

def recommend_movies(title, cosine_sim=cosine_sim):
    # Dapatkan indeks film berdasarkan judul
    idx = movies_sampled[movies_sampled['title'] == title].index[0]

    # Dapatkan skor similarity untuk semua film
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Urutkan film berdasarkan skor similarity tertinggi
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Ambil 10 film teratas selain film itu sendiri
    sim_scores = sim_scores[1:11]

    # Dapatkan judul, genre, dan skor similarity dari indeks
    movie_indices = [i[0] for i in sim_scores]
    recommended_movies = movies_sampled.iloc[movie_indices].copy()
    recommended_movies['similarity_score'] = [i[1] for i in sim_scores]

    return pd.DataFrame(recommended_movies[['movieId', 'title', 'genres', 'similarity_score']])

df_recommendations = recommend_movies('Toy Story (1995)')
df_recommendations

"""### Collaborative Filtering

Assign ratings_sampled ke variabel df
"""

df = ratings_sampled
df

"""Proses ini dilakukan untuk mengubah userId menjadi daftar unik, kemudian melakukan encoding untuk mengonversi userId menjadi angka, serta menyediakan cara untuk mendekode angka kembali ke userId asli."""

# Mengubah userId menjadi list tanpa nilai yang sama
user_ids = df['userId'].unique().tolist()
print('list userId: ', user_ids)

# Melakukan encoding userId
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userId : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userId
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userId: ', user_encoded_to_user)

"""Selanjutnya mengubah movieId menjadi daftar unik, kemudian melakukan encoding menjadi angka, serta menyediakan cara untuk mendekode angka kembali ke movieId asli."""

# Mengubah movieId menjadi list tanpa nilai yang sama
movie_ids = df['movieId'].unique().tolist()

# Melakukan proses encoding movieId
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}

# Melakukan proses encoding angka ke movieId
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

"""Selanjutnya memetakan userId dan movieId ke nilai numerik yang telah di-encode, dengan mengganti kolom userId dan movieId di DataFrame df menggunakan dictionary encoding yang telah dibuat sebelumnya."""

# Mapping userId ke dataframe user
df['user'] = df['userId'].map(user_to_user_encoded)

# Mapping movieId ke dataframe movie
df['movie'] = df['movieId'].map(movie_to_movie_encoded)

"""Kemudian kita akan menghitung jumlah user dan movie unik, mengonversi rating menjadi tipe data float, serta menentukan nilai minimum dan maksimum rating untuk normalisasi, lalu mencetak informasi tersebut."""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah movie
num_movie = len(movie_encoded_to_movie)
print(num_movie)

# Mengubah rating menjadi nilai float
df['rating'] = df['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['rating'])

# Nilai maksimal rating
max_rating = max(df['rating'])

print('Number of User: {}, Number of Movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movie, min_rating, max_rating
))

"""Selanjutnya kita akan mengacak dataset. ini dilakukan untuk memastikan model tidak terpengaruh oleh urutan data yang mungkin mengandung pola atau bias tertentu, sehingga model dapat belajar dari distribusi data yang lebih representatif dan tidak hanya menghafal urutan data."""

df = df.sample(frac=1, random_state=42)
df

"""Pada tahap ini, data dibagi menjadi fitur (x) yang berisi pasangan (user, movie) dan target (y) yang berisi rating yang telah dinormalisasi, lalu dibagi menjadi 80% untuk data pelatihan dan 20% untuk data validasi."""

# Membuat variabel x untuk mencocokkan data user dan movie menjadi satu value
x = df[['user', 'movie']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""Membuat fungsi RecommenderNet, yang digunakan untuk rekomendasi berbasis Neural Collaborative Filtering (NCF) menggunakan TensorFlow"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.movie_embedding = layers.Embedding( # layer embeddings movie
        num_movie,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_movie, 1) # layer embedding movie bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    movie_vector = self.movie_embedding(inputs[:, 1]) # memanggil layer embedding 3
    movie_bias = self.movie_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)

    x = dot_user_movie + user_bias + movie_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""Melakukan inisialisasi model dengan parameter sebagai berikut:
- loss menggunakan binary consentropy
- optimizer menggunakan Adam dengan learning_rate = 0.001
- metrik menggunakan RMSE
"""

model = RecommenderNet(num_users, num_movie, 50)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Selanjutnya, lakukan pelatihan pada model dengan 100 epoch dan batch size sebanyak 8, serta lakukan validasi selama proses pelatihan."""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""Menyiapkan data untuk memprediksi rekomendasi film bagi user tertentu"""

movie_df = movies_sampled
df = ratings_sampled

# Mengambil sample user
user_id = df.userId.sample(1).iloc[0]
movie_visited_by_user = df[df.userId == user_id]

movie_not_visited = movie_df[~movie_df['movieId'].isin(movie_visited_by_user.movieId.values)]['movieId']
movie_not_visited = list(
    set(movie_not_visited)
    .intersection(set(movie_to_movie_encoded.keys()))
)

movie_not_visited = [[movie_to_movie_encoded.get(x)] for x in movie_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_visited), movie_not_visited)
)
print(user_movie_array)

"""Membuat prediksi top 10 rekomedasi film berdasarkan rating yang diberikan pengguna"""

ratings = model.predict(user_movie_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('movie with high ratings from user')
print('----' * 8)

top_movie_user = (
    movie_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)

movie_df_rows = movie_df[movie_df['movieId'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.title, ':', row.genres)

print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)

recommended_movie = movie_df[movie_df['movieId'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.title, ':', row.genres)

"""### Evaluasi

Melakukan evaluasi terhadap dua metode rekomendasi, yaitu Content-Based Filtering dengan menggunakan metrik MAE dan RMSE, serta Collaborative Filtering dengan metrik Precision, untuk menilai kualitas hasil rekomendasi yang dihasilkan oleh masing-masing model.
"""

# Ambil genre Toy Story (1995) sebagai string
toy_story_genres = movies_sampled[movies_sampled['title'] == 'Toy Story (1995)']['genres'].values[0]

# Hitung relevansi setiap rekomendasi
relevan_count = 0

for index, row in df_recommendations.iterrows():
    recommended_movie_genres = row['genres']  # Genre film yang direkomendasikan


    # Cek apakah ada genre yang cocok antara Toy Story dan rekomendasi
    if any(genre in toy_story_genres for genre in recommended_movie_genres):
        relevan_count += 1

print(relevan_count)
precision_cb = relevan_count / 10  # Total 10 rekomendasi

# Prediksi rating pada validasi untuk Collaborative Filtering
y_pred = model.predict(x_val).flatten()

# Evaluasi Collaborative Filtering
mae_cf = mean_absolute_error(y_val, y_pred)
rmse_cf = np.sqrt(mean_squared_error(y_val, y_pred))

# Hasil evaluas
print(f"Content-Based Filtering - Precision: {precision_cb * 100:.2f}%")
print(f"Collaborative Filtering - MAE: {mae_cf:.4f}, RMSE: {rmse_cf:.4f}")